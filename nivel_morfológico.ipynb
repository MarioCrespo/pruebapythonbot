{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9JOhR0tUeulPjpaXfx4kO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioCrespo/pruebapythonbot/blob/master/nivel_morfol%C3%B3gico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dos conceptos clave en NLTK y en el NLP en general son la tokenización y el stemming.\n",
        "\n",
        "*Tokenización*: La tokenización es el proceso de dividir un texto en unidades más pequeñas, llamadas tokens. Un token puede ser una palabra, una oración, un párrafo, un símbolo o un signo de puntuación, dependiendo del contexto y del tipo de tokenización que se realice. La tokenización es un paso fundamental en el procesamiento del lenguaje natural, ya que facilita el análisis del texto al dividirlo en partes más manejables. En NLTK, hay varias funciones de tokenización disponibles, como **word_tokenize()** para dividir texto en palabras y **sent_tokenize()** para dividirlo en oraciones.\n"
      ],
      "metadata": {
        "id": "v8_rCf6siJes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La tokenización de palabras en nltk se realiza con **word_tokenize()**, la de oraciones con **sent_tokenize()** . Para ello escribiremos el siguiente código:\n",
        "\n",
        "***from nltk import word_tokenize, sent_tokenize()***\n",
        "\n",
        "Adicionalmente cargaremos el paquete \"punk\":\n",
        "\n",
        "***nltk.download('punkt')***\n",
        "\n",
        "Este paquete contiene un modelo pre-entrenado que es esencial para realizar la tokenización de oraciones y palabras en el texto. El modelo \"punkt\" es un tokenizador no supervisado basado en el aprendizaje automático que se entrena para reconocer abreviaturas, signos de puntuación y caracteres especiales que indican el final de una oración o el inicio de una nueva. Al descargar e instalar este paquete, le permitirá utilizar funciones de tokenización, como nltk.sent_tokenize() y nltk.word_tokenize() para dividir el texto en oraciones y palabras, respectivamente.\n",
        "\n"
      ],
      "metadata": {
        "id": "nxcKLhh6i7pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlqwvTCOi6Lz",
        "outputId": "1c33dbc6-f4f1-4db6-b5aa-77a25ea67250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez instalado, se puede usar:"
      ],
      "metadata": {
        "id": "NRsVfCAAi6pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"Ya queda poco para terminar la asignatura. Al menos eso creo.\"\n",
        "tokens = word_tokenize(string.lower())\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9hXOJy-iPnN",
        "outputId": "916feb2d-be4b-47ec-8ab2-affe90c08b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ya', 'queda', 'poco', 'para', 'terminar', 'la', 'asignatura', '.', 'al', 'menos', 'eso', 'creo', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compara el resultado anterior con el que hemos estado usando hasta ahora:"
      ],
      "metadata": {
        "id": "7m8_2m0okz-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.split())#dividimos el texto en tokens a partir de espacio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTEBv8qhk6gL",
        "outputId": "9f6c73ee-a2dc-4bef-d990-618a08e22cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ya', 'queda', 'poco', 'para', 'terminar', 'la', 'asignatura.', 'Al', 'menos', 'eso', 'creo.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿ En qué se diferencia word_tokenize(string) de string.split() ?"
      ],
      "metadata": {
        "id": "mJpJ4bSUlEsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anteriormente, cargábamos los textos en el nltk a partir de .split, pero word_tokenize también produce una lista. Veamos:"
      ],
      "metadata": {
        "id": "Rpm_cjSym5CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_periodistico = '''Una nueva era se ha abierto en la Unión Europea (UE). Y, por tanto, en España. Las subidas de los tipos de interés del Banco Central Europeo (BCE) y las nuevas reglas fiscales amenazan las políticas sociales de los gobiernos para luchar contra el impacto de la inflación y las consecuencias de la guerra de Ucrania. La combinación del encarecimiento de la financiación para luchar contra la subida de precios y la vuelta a normas de control presupuestario para reducir los endeudamientos atan las 'manos' de los ejecutivos europeos para diseñar medidas que alivien el daño que sufren los ingresos reales de las familias.\n",
        "\n",
        "La cuestión es que, efectivamente, los planes anticrisis (bajadas de impuestos, descuentos en el transporte, tope al gas...) vienen amortiguando el golpe de las subidas precios, y también de la estrategia de la política monetaria. Porque, con las subidas de tipos, el BCE pretende reducir la capacidad de consumo de las familias, de inversión de las empresas y, por supuesto, de gasto del Estado (al que se le incrementa la factura de intereses de la deuda pública). Básicamente, el objetivo es desacelerar la economía en general para contener así la inflación, pese al riesgo de recesión y de que aumente el paro.\n",
        "\n",
        "Judith Arnal, investigadora del Real Instituto Elcano, señala que “el mayor problema que puede llegar a haber” con esta descoordinación entre las medidas de los gobiernos y la política monetaria del BCE es que “una política fiscal expansiva contrarresta los efectos del endurecimiento de la política monetaria, forzando entonces al banco central a subir tipos por encima del nivel que resultaría óptimo”. Ese escenario significa más sufrimiento y más desigualdad, al ser siempre más vulnerables las familias más pobres, las empresas pequeñas y medianas que necesitan financiación y los estados más sobreendeudados, como son España o Italia.\n",
        "\n",
        "Por su parte, Víctor Alvargonzález, socio fundador de Nextep Finance, incide en que “lo peor es que la política monetaria no se anula del todo [con políticas fiscales expansivas]. A quienes le suben los tipos de interés, pero no reciben una subvención, ni les suben los sueldos, ni les dan obra pública para sus empresas... solo les queda la parte mala. Mientras que quienes reciben apoyo del Estado de una u otra forma les afecta menos que les suban los tipos de interés en los créditos hipotecarios”.'''"
      ],
      "metadata": {
        "id": "gxs9coe6nF-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import Text\n",
        "texto_en_nltk = nltk.Text(texto_periodistico.split())\n",
        "len(texto_en_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXeUEdg3nm_L",
        "outputId": "06565852-73d8-4475-ec3b-0c9f770a351b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "390"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto_en_nltk = nltk.Text(word_tokenize(texto_periodistico))\n",
        "len(texto_en_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRPKzOgUoILJ",
        "outputId": "b5005eb1-52b3-4eeb-e964-9d1826a6dd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "447"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿los resultado de los dos anteriores no son iguales, por qué?"
      ],
      "metadata": {
        "id": "pq93YaUPoW7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_en_nltk = nltk.Text(word_tokenize(texto_periodistico))\n",
        "filtered_words = [word for word in texto_en_nltk if word not in '''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~''']\n",
        "\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgHB0rz-pHW7",
        "outputId": "0ba0af9a-8927-4c73-f84b-6746f3b12e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Una', 'nueva', 'era', 'se', 'ha', 'abierto', 'en', 'la', 'Unión', 'Europea', 'UE', 'Y', 'por', 'tanto', 'en', 'España', 'Las', 'subidas', 'de', 'los', 'tipos', 'de', 'interés', 'del', 'Banco', 'Central', 'Europeo', 'BCE', 'y', 'las', 'nuevas', 'reglas', 'fiscales', 'amenazan', 'las', 'políticas', 'sociales', 'de', 'los', 'gobiernos', 'para', 'luchar', 'contra', 'el', 'impacto', 'de', 'la', 'inflación', 'y', 'las', 'consecuencias', 'de', 'la', 'guerra', 'de', 'Ucrania', 'La', 'combinación', 'del', 'encarecimiento', 'de', 'la', 'financiación', 'para', 'luchar', 'contra', 'la', 'subida', 'de', 'precios', 'y', 'la', 'vuelta', 'a', 'normas', 'de', 'control', 'presupuestario', 'para', 'reducir', 'los', 'endeudamientos', 'atan', 'las', \"'manos\", 'de', 'los', 'ejecutivos', 'europeos', 'para', 'diseñar', 'medidas', 'que', 'alivien', 'el', 'daño', 'que', 'sufren', 'los', 'ingresos', 'reales', 'de', 'las', 'familias', 'La', 'cuestión', 'es', 'que', 'efectivamente', 'los', 'planes', 'anticrisis', 'bajadas', 'de', 'impuestos', 'descuentos', 'en', 'el', 'transporte', 'tope', 'al', 'gas', '...', 'vienen', 'amortiguando', 'el', 'golpe', 'de', 'las', 'subidas', 'precios', 'y', 'también', 'de', 'la', 'estrategia', 'de', 'la', 'política', 'monetaria', 'Porque', 'con', 'las', 'subidas', 'de', 'tipos', 'el', 'BCE', 'pretende', 'reducir', 'la', 'capacidad', 'de', 'consumo', 'de', 'las', 'familias', 'de', 'inversión', 'de', 'las', 'empresas', 'y', 'por', 'supuesto', 'de', 'gasto', 'del', 'Estado', 'al', 'que', 'se', 'le', 'incrementa', 'la', 'factura', 'de', 'intereses', 'de', 'la', 'deuda', 'pública', 'Básicamente', 'el', 'objetivo', 'es', 'desacelerar', 'la', 'economía', 'en', 'general', 'para', 'contener', 'así', 'la', 'inflación', 'pese', 'al', 'riesgo', 'de', 'recesión', 'y', 'de', 'que', 'aumente', 'el', 'paro', 'Judith', 'Arnal', 'investigadora', 'del', 'Real', 'Instituto', 'Elcano', 'señala', 'que', '“', 'el', 'mayor', 'problema', 'que', 'puede', 'llegar', 'a', 'haber', '”', 'con', 'esta', 'descoordinación', 'entre', 'las', 'medidas', 'de', 'los', 'gobiernos', 'y', 'la', 'política', 'monetaria', 'del', 'BCE', 'es', 'que', '“', 'una', 'política', 'fiscal', 'expansiva', 'contrarresta', 'los', 'efectos', 'del', 'endurecimiento', 'de', 'la', 'política', 'monetaria', 'forzando', 'entonces', 'al', 'banco', 'central', 'a', 'subir', 'tipos', 'por', 'encima', 'del', 'nivel', 'que', 'resultaría', 'óptimo', '”', 'Ese', 'escenario', 'significa', 'más', 'sufrimiento', 'y', 'más', 'desigualdad', 'al', 'ser', 'siempre', 'más', 'vulnerables', 'las', 'familias', 'más', 'pobres', 'las', 'empresas', 'pequeñas', 'y', 'medianas', 'que', 'necesitan', 'financiación', 'y', 'los', 'estados', 'más', 'sobreendeudados', 'como', 'son', 'España', 'o', 'Italia', 'Por', 'su', 'parte', 'Víctor', 'Alvargonzález', 'socio', 'fundador', 'de', 'Nextep', 'Finance', 'incide', 'en', 'que', '“', 'lo', 'peor', 'es', 'que', 'la', 'política', 'monetaria', 'no', 'se', 'anula', 'del', 'todo', 'con', 'políticas', 'fiscales', 'expansivas', 'A', 'quienes', 'le', 'suben', 'los', 'tipos', 'de', 'interés', 'pero', 'no', 'reciben', 'una', 'subvención', 'ni', 'les', 'suben', 'los', 'sueldos', 'ni', 'les', 'dan', 'obra', 'pública', 'para', 'sus', 'empresas', '...', 'solo', 'les', 'queda', 'la', 'parte', 'mala', 'Mientras', 'que', 'quienes', 'reciben', 'apoyo', 'del', 'Estado', 'de', 'una', 'u', 'otra', 'forma', 'les', 'afecta', 'menos', 'que', 'les', 'suban', 'los', 'tipos', 'de', 'interés', 'en', 'los', 'créditos', 'hipotecarios', '”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Esta es una oración de ejemplo para demostrar cómo combinar for e if en Python.\"\n",
        "words = word_tokenize(text)\n",
        "words_starting_with_e = [word for word in words if word.lower().startswith('e')]\n",
        "print(words_starting_with_e)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4RX6X5Atztz",
        "outputId": "e68bb207-89c2-4446-c1a0-65131c63dd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Esta', 'es', 'ejemplo', 'e', 'en']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a ver ahora **sent_tokenize()**:"
      ],
      "metadata": {
        "id": "fevYX0YulPlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "oraciones = sent_tokenize(string)\n",
        "print(oraciones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpvdBgsJlUH6",
        "outputId": "be265b9a-d28c-427a-c67a-e406e922a354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ya queda poco para terminar la asignatura.', 'Al menos eso creo.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando hemos ejecutado **sent_tokenize()**, ¿qué tipo de dato es el output?"
      ],
      "metadata": {
        "id": "yzCNcNPcmMwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WYy5uyCKm1O2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcxvvhSYmEG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK puede etiquetar clases de palabras, pero solo para el inglés. Para el español podemos utilizar otras librerías como spaCy:\n",
        "\n",
        "Se trata de una biblioteca de procesamiento del lenguaje natural (NLP, por sus siglas en inglés) de código abierto desarrollada en Python. Fue creada por Explosion AI y es ampliamente utilizada en la comunidad de NLP y análisis de texto. La biblioteca ofrece funciones eficientes y de alto rendimiento para realizar tareas comunes de NLP, como tokenización, etiquetado gramatical, análisis de dependencias o reconocimiento de entidades\n"
      ],
      "metadata": {
        "id": "_gj2nmlrWau2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlmXuLi-WVAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53759a12-3d8c-4bf8-d0b2-8c01ecfc61e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog.', 'NN')]\n",
            "('The', 'DT')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tags = nltk.pos_tag(text.split())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tags)\n",
        "etiquetas = tags\n",
        "print(etiquetas[0])\n",
        "\n",
        "print(etiquetas[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIl3bHIZnaV-",
        "outputId": "14588737-6f7d-4488-a7c2-18e30f8e042a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog.', 'NN')]\n",
            "('The', 'DT')\n",
            "DT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numero = 0\n",
        "for elemento, valor in etiquetas:\n",
        "  print(elemento, valor)\n",
        "  if valor == \"JJ\":\n",
        "    numero += 1\n",
        "\n",
        "print(\"el resultado de NN es\", numero)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egTBn4UZnuwC",
        "outputId": "4fd7b9fb-fa27-42d0-ab3c-a5086c177aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DT\n",
            "quick JJ\n",
            "brown NN\n",
            "fox NN\n",
            "jumps VBZ\n",
            "over IN\n",
            "the DT\n",
            "lazy JJ\n",
            "dog. NN\n",
            "el resultado de NN es 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿De qué manera opera este clasificador? El método pos_tag de NLTK es un clasificador gramatical que utiliza aprendizaje automático. Mediante el análisis de miles de ejemplos de enunciados con etiquetas manuales, el sistema ha sido capaz de aprender, determinar frecuencias y establecer cuál es la clase gramatical con mayor probabilidad para cada elemento en la oración."
      ],
      "metadata": {
        "id": "jdmmKYpFfs1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download es_core_news_md #nos descargamos su modelo de etiquetado para español\n",
        "nlp = spacy.load('es_core_news_md')\n"
      ],
      "metadata": {
        "id": "xhqSDQ0FWgaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57826406-3225-45dc-8eeb-209c1164b55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-05 11:21:26.804906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-md==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.5.0/es_core_news_md-3.5.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-md==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (23.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.1.2)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(nlp)"
      ],
      "metadata": {
        "id": "Ugro57ElqJTY",
        "outputId": "01498429-695c-44f9-ad64-e81342a1bd97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.es.Spanish"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo es_core_news_md de SpaCy es un modelo estadístico entrenado utilizando el conjunto de datos \"AnCora\". AnCora es un corpus anotado del español que contiene textos periodísticos con anotaciones lingüísticas, como etiquetas de partes del discurso, análisis morfosintáctico y relaciones semánticas."
      ],
      "metadata": {
        "id": "nrhU5IiHhEgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"juan come las patatas que María le dio.\") \n",
        "print(doc[0])\n",
        "print(doc[0].lemma_)\n",
        "print(doc[0].pos_)\n",
        "print(doc[0].dep_)\n",
        "print()\n",
        "for token in doc:\n",
        "  print(token, token.lemma_, token.pos_, token.dep_)#se va a imprimir el token, su lema, su clase de palabra y su función sintáctica\n",
        "#la lista de etiquetas está aquí https://www.kaggle.com/code/weka511/list-spacy-tags"
      ],
      "metadata": {
        "id": "hCUKE7TuWjZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4334a7c-d158-4c42-87d7-d649e71569c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "juan\n",
            "juan\n",
            "PROPN\n",
            "nsubj\n",
            "\n",
            "juan juan PROPN nsubj\n",
            "come come PROPN ROOT\n",
            "las el DET det\n",
            "patatas patata NOUN obj\n",
            "que que PRON obj\n",
            "María María PROPN nsubj\n",
            "le él PRON obj\n",
            "dio dar VERB acl\n",
            ". . PUNCT punct\n"
          ]
        }
      ]
    }
  ]
}